---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
- Kazuki Yano is a first-year Ph.D. student in Information Science, working on machine learning at [Tohoku NLP](https://www.nlp.ecei.tohoku.ac.jp/).
- I’m interested in Natural Language Processing (NLP) and other related technologies.
- I'm focusing on efficient training methods for large
language models.

---

# Work experience
* November 2024 - Current: SB Intuitions.
  * Research Intern.
* August 2024 - March 2025: Preferred Networks Inc.
  * Reseach Intern.
* July 2024 - May 2025: AI Architecture Section, AI for Business Department,Rakuten Group, Inc.
  * Data Scientist (Part-time job).
* April 2024 - Present: [Graduate Program in Data Science](https://gp-ds.tohoku.ac.jp/en/index.html)
  * Research Assistant.
* August 2023 - September 2023: LINE Corporation
  * Reseach Intern.
  * Focused on research on large language models, specifically adversarial attacks against language models.
  * see [achievement](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P6-14.pdf)

# Publication

## International Conference
### 2025
- **Kazuki Yano**, Sho Takase, Sosuke Kobayashi, Shun Kiyono, Jun Suzuki. Efficient Construction of Model Family through Progressive Training Using Model Expansion. preprint. [link](https://arxiv.org/abs/2504.00623)
- **Kazuki Yano**, Takumi Ito, Jun Suzuki.  STEP: Staged Parameter-Efficient Pre-training for Large Language Models. Proceedings of the 2025 Conference of the Nations of American Chapter of the Association for Computational Linguistics: Human Language Technologies(NAACL 2025) April 2025. [link](https://aclanthology.org/2025.naacl-short.32/).
### 2024
- Keito Kudo, Hiroyuki Deguchi, Makoto Morishita, Ryo Fujii, Takumi Ito, Shintaro Ozaki, Koki Natsumi, Kai Sato, **Kazuki Yano**, Ryosuke Takahashi, Subaru Kimura, Tomomasa Hara, Yusuke Sakai, Jun Suzuki. Document-level Translation with LLM Reranking: Team-J at WMT 2024 General Translation Task. In Proceedings of the Ninth Conference on Machine Translation, pages 210–226, Miami, Florida, USA. Association for Computational Linguistics.


## Domestic Conference
- **矢野一樹**, 伊藤拓海, 鈴木潤. モデル拡張によるパラメータ効率的なLLMの事前学習. 言語処理学会第31回年次大会(NLP 2025), March 2025.
- **矢野一樹**, 高瀬翔, 小林颯介, 清野舜, 鈴木潤. モデル拡張を用いた段階的事前学習によるモデル系列の効率的な構築. 言語処理学会第31回年次大会(NLP 2025), March 2025.
- 李宰成, **矢野一樹**, 高橋良允, 柴田圭悟, 池田航, 鈴木潤. LLM 事前学習の効率化と性質改善:埋め込み層および出力層のパラメータ固定による再利用. 言語処理学会第 31 回年次大会(NLP 2025), March 2025.
- 池田航, **矢野一樹**, 高橋良允, 李宰成, 柴田圭悟, 鈴木潤. Transformer LLM における層単位のFFN層の重要度検証. 言語処理学会第31回年次大会(NLP 2025), March 2025.
- 柴田圭悟, 高橋良允, **矢野一樹**,李宰成, 池田航, 鈴木潤. Transformer LLMの内部挙動改善:隠れ状態ベクトルの数値的収束性の向上. 言語処理学会第31回年次大会(NLP 2025), March 2025.
- 高橋良允, **矢野一樹**, 成瀬健太, 武井美緒, 梶佑輔, 鈴木潤. LLMにおける内部表現を用いた日本語スタイル制御メカニズムの分析. 言語処理学会第31回年次大会(NLP 2025), March 2025.
- **矢野一樹** (東北大学), 綿岡晃輝, Thien Q. Tran, 髙橋翼, Seng Pei Liew (LINEヤフー), 鈴木潤 (東北大/理研). 対話モデルに対する敵対的プロンプトの効率的な最適化. 言語処理学会第30回年次大会(NLP 2024), March 2024.
- **矢野一樹** (東北大学), 伊藤拓海 (東北大/Langsmith), 鈴木潤 (東北大/理研). LoRAチューニングの効果的な利用のための設定分析. NLP若手の会 第18回シンポジウム. August 2023.
- **矢野一樹**, ジェプカ ラファウ, 荒木健治. 日本語知識グラフを用いた質問応答システムの構築とグラフサイズ
の影響の検証. 情報処理学会 第255回自然言語処理研究会, March 2023. 

## Awards
- [言語処理学会第31回年次大会(NLP2025), 優秀賞](https://arc.net/l/quote/kelkrwml). Rate 上位14/765件
- [言語処理学会第31回年次大会(NLP2025), 若手奨励賞(共著)](https://arc.net/l/quote/zmgklkby). Rate 上位20/487件
- [言語処理学会第30回年次大会(NLP2024), 若手奨励賞](https://www.anlp.jp/nlp2024/award.html#P6-14). Rate 上位18/427件

## Education
* Ph.D. in Information Science, Tohoku University, 2028 (expected)
  * Supervisor: [Prof. Jun Suzuki](https://www.fai.cds.tohoku.ac.jp/members/js/)
* M.S. in Information Science, Tohoku University, 2025
* B.S. in Information Science, Hokkaido University, 2023

## Skills
* Python: fluent
* Rust: intermediate
* Statistics/Data Science
  * Level 1 Certificate in Mathematical Statistics, Japan
